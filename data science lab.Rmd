## EMPIRICAL ANALYSIS: Linear Models 

# Correlation Heatmap Raw
```{r}
# First, I selected only the numeric columns from the dataset 
# so I can compute meaningful pairwise correlations
all_car_adverts_num <- all_car_adverts %>% select_if(is.numeric)

# Then I calculated the correlation matrix, using pairwise.complete.obs 
# to handle missing data without biasing the results
cor_matrix <- cor(all_car_adverts_num, use = "pairwise.complete.obs")

# To visualize the correlation structure, I reshaped the matrix to long format
cor_melted <- melt(cor_matrix)

# I used a heatmap to explore linear associations among numeric variables
# Blue = negative correlation, Red = positive
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Correlation Heatmap (Numerical Variables Only)", fill = "Correlation")

```

# Correlation Heatmap Filtered
```{r}
# Here, I removed columns with zero variance since they provide no predictive value
all_car_adverts_num <- all_car_adverts_num %>%
  select(where(~ var(.) > 0))

# I recomputed the correlation matrix with filtered variables
cor_matrix <- cor(all_car_adverts_num, use = "pairwise.complete.obs")

# To avoid gray boxes in the heatmap, I removed rows/columns with NA values
cor_matrix <- cor_matrix[complete.cases(cor_matrix), complete.cases(cor_matrix)]

# I kept only variables with moderate-to-strong correlation (|r| ≥ 0.1) with car_price
cor_filtered <- cor_matrix[abs(cor_matrix["car_price", ]) >= 0.1, abs(cor_matrix["car_price", ]) >= 0.1]

# Melted again for heatmap visualization
cor_melted <- melt(cor_filtered)

# Final refined heatmap to focus on car_price-related variables
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Filtered Correlation Heatmap", fill = "Correlation")

```

# Pre-Processing the Data for Linear Models

```{r}
# I started by preparing the dataset for linear model training.
# These columns below didn’t offer much predictive power or were mostly sparse/duplicated,
# so I decided to remove them to simplify the modeling process.
lineardatatrain <- remaining_data %>%
  select(-c(
    full_dealership,         # Too sparse; most values are NA or uniform
    first_year_road_tax,     # Low variance and not very informative
    part_warranty,           # Often redundant with full_service or part_service
    part_service,
    full_service,
    finance_available        # Usually not known at prediction time
  ))

# I applied the same column exclusions to the holy test set
# to ensure consistency in structure between training and evaluation data.
holy_test_data <- final_test_set %>%
  select(-c(
    full_dealership,
    first_year_road_tax,
    part_warranty,
    part_service,
    full_service,
    finance_available
  ))

```



# Selecting Only The Most Occurring Brands

```{r}
# Selecting Only The Most Occurring Brands

# I started by counting how many times each car brand appears in the training dataset.
brand_counts <- table(lineardatatrain$make)

# Then I converted the result to a data frame and sorted it in descending order of frequency.
brand_counts_df <- as.data.frame(brand_counts) %>%
  arrange(desc(Freq))

# After some trial and error, I found that including the top 46 brands gave the best trade-off.
# This was the highest number of brands that consistently worked across all models without issues.
# Brands beyond the 46th were extremely rare, and keeping them often led to instability or poor performance.
top_46_brands <- brand_counts_df$Var1[1:46]

# I filtered both the training and holy test sets to keep only those most frequent brands.
# This keeps the datasets aligned and avoids introducing noise from underrepresented categories.
lineardatatrain <- lineardatatrain %>% filter(make %in% top_46_brands)
holy_test_data  <- holy_test_data %>% filter(make %in% top_46_brands)

```

# Train / Validate / Test: Linear Models

```{r}
# ----------------------------
# Sample 200,000 Observations from Trainable Data and Create Train and Validation Data 
# ----------------------------

# I randomly sampled 200,000 observations from the training dataset without replacement.
# This helped me control the model runtime and ensured consistency across different model types.
sampled_datalinear <- lineardatatrain %>%
  sample_n(200000, replace = FALSE)           

# I then created a 70-30 train-validation split using stratified sampling based on log_price.
# This helped preserve the distribution of the target variable across both sets.
train_index <- createDataPartition(sampled_datalinear$log_price, p = 0.7, list = FALSE)

# Here, I split the sampled data into training and validation sets.
train_data <- sampled_datalinear[train_index, ]
validation_data <- sampled_datalinear[-train_index, ]

# I then merged the training and validation sets to create a full dataset that I use for cross-validation and final model fitting.
train_val_data <- bind_rows(train_data, validation_data)

# For cross-validation, I set up a 5-fold CV strategy to ensure more stable performance estimates across all linear models.
cv_control <- trainControl(method = "cv", number = 5)

```


# Simple Linear Regression

```{r}
# ----------------------------
# LOG VERSION
# ----------------------------

# I excluded variables that either contain the outcome (log_price) or are not suitable for modeling, such as identifiers and high-cardinality columns.
excluded_predictors_log <- c("car_price", "log_price", "model", "variant", "luxury_level")
predictors_log <- setdiff(names(train_val_data), excluded_predictors_log)

# I constructed the formula for log-transformed car price prediction using all selected predictors.
lm_formula_log <- as.formula(paste("log_price ~", paste(predictors_log, collapse = " + ")))

# I trained a linear regression model with 5-fold cross-validation to estimate performance on unseen data.
lm_cv_model_log <- train(
  lm_formula_log,
  data = train_val_data,
  method = "lm",
  trControl = cv_control
)

# I computed the average cross-validated MSE and R² to assess performance stability.
lm_mse_cv_log <- mean(lm_cv_model_log$resample$RMSE^2)
lm_r2_cv_log  <- mean(lm_cv_model_log$resample$Rsquared)

# Then, I fitted the final model on the full train+validation set to maximize data usage.
final_lm_model_log <- lm(lm_formula_log, data = train_val_data)

# Using this final model, I predicted log-prices on the untouched holy test set.
lm_preds_log_holy  <- predict(final_lm_model_log, newdata = holy_test_data)

# Finally, I evaluated model accuracy and explained variance on the test set.
lm_mse_holy_log  <- mean((holy_test_data$log_price - lm_preds_log_holy)^2)
lm_r2_holy_log   <- cor(holy_test_data$log_price, lm_preds_log_holy)^2

```

```{r}
# ----------------------------
# ACTUAL VERSION
# ----------------------------

# I repeated the same steps for predicting actual car prices instead of log-transformed prices.
excluded_predictors_actual <- c("log_price", "car_price", "model", "variant", "luxury_level")
predictors_actual <- setdiff(names(train_val_data), excluded_predictors_actual)
lm_formula_actual <- as.formula(paste("car_price ~", paste(predictors_actual, collapse = " + ")))

# Again, I used 5-fold CV to train and evaluate the model.
lm_cv_model_actual <- train(
  lm_formula_actual,
  data = train_val_data,
  method = "lm",
  trControl = cv_control
)

# Collecting CV metrics for comparison.
lm_mse_cv_actual <- mean(lm_cv_model_actual$resample$RMSE^2)
lm_r2_cv_actual  <- mean(lm_cv_model_actual$resample$Rsquared)

# I fit the final model on the full train+val dataset.
final_lm_model_actual <- lm(lm_formula_actual, data = train_val_data)

# And made predictions on the holy test set.
lm_preds_holy_actual  <- predict(final_lm_model_actual, newdata = holy_test_data)

# Test set evaluation: MSE and R².
lm_mse_holy_actual <- mean((holy_test_data$car_price - lm_preds_holy_actual)^2)
lm_r2_holy_actual  <- cor(holy_test_data$car_price, lm_preds_holy_actual)^2

```

# Results of Simple Linear Regression

```{r}
# I organized the results into a single summary table for both log and actual price models.
lm_eval_table <- data.frame(
  Version = rep(c("Log Price", "Actual Price"), each = 2),
  Evaluation = rep(c("Cross-Validation (5-Fold)", "Holy Test Set"), 2),
  RMSE = c(sqrt(lm_mse_cv_log), sqrt(lm_mse_holy_log),
           sqrt(lm_mse_cv_actual), sqrt(lm_mse_holy_actual)),
  R2 = c(lm_r2_cv_log, lm_r2_holy_log,
         lm_r2_cv_actual, lm_r2_holy_actual)
)

# I rounded the metrics for better readability.
lm_eval_table_clean <- lm_eval_table %>%
  mutate(across(c(RMSE, R2), ~ round(.x, 3)))

# Finally, I displayed the table in a clean HTML format for reporting.
lm_eval_table_clean %>%
  kable("html", caption = "Linear Regression Performance (RMSE and R²) — All Evaluation Sets", align = "c") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = FALSE, position = "center") %>%
  row_spec(0, bold = TRUE, color = "white", background = "#4E79A7")

```


# Best Subset Selection

```{r}
# --- BEST SUBSET SELECTION: ACTUAL PRICE VERSION ---
# Note: I chose to model the actual `car_price` rather than `log_price` for Best Subset Selection,
# as well as for the Lasso and Ridge models. While log transformation is useful for stabilizing variance 
# and improving normality in simple linear models, it makes the interpretability of results a bit trickier.
# Since both regularization and subset selection are already designed to handle variance and overfitting,
# I felt it was more meaningful to predict the actual prices — especially given our goal of comparing
# raw predictive performance across models and communicating findings in real monetary terms.


# Before starting, I made sure that the function 'generate_formulas()' has been defined.

# I filtered out the 'panel van' body type since it’s a rare case and can cause instability in model selection.
train_val_data <- train_val_data %>%
  filter(!body_type %in% "panel van")

# I set my prediction target and excluded variables that are either direct outcomes or problematic for modeling (e.g., high-cardinality or redundant).
target_bss_actual <- "car_price"
excluded_bss_actual <- c("log_price", "car_price", "model", "variant", "reg", "luxury_level")
predictors_bss_actual <- setdiff(names(train_val_data), excluded_bss_actual)

# I created an empty results table to store cross-validation metrics and formulas for each subset size.
bss_results_actual <- data.frame(
  predictors_count = integer(),
  formula = character(),
  cv_mse = numeric(),
  cv_r2 = numeric(),
  stringsAsFactors = FALSE
)

# I looped through a predefined range of subset sizes (predictor_range) and evaluated all combinations.
for (p in predictor_range) {
  formulas <- generate_formulas(p, predictors_bss_actual, target_bss_actual)
  for (f in formulas) {
    metrics <- evaluate_formula_cv(f, train_val_data)
    bss_results_actual <- rbind(bss_results_actual, data.frame(
      predictors_count = p,
      formula = f,
      cv_mse = metrics$mean_mse,
      cv_r2 = metrics$mean_r2
    ))
  }
}

# I didn’t want to simply choose the formula with the lowest MSE or highest R², since that tends to include *all* predictors—defeating the purpose of subset selection.
# So I built a custom scoring function that balances prediction error, goodness of fit, and model simplicity.
# I gave 47.5% weight to normalized RMSE, 47.5% to 1 - normalized R² (so higher R² leads to lower penalty),
# and a smaller 5% penalty to the number of predictors — to reward more parsimonious models.
bss_results_actual <- bss_results_actual %>%
  mutate(score = 0.475 * (cv_mse / max(cv_mse)) +
                 0.475 * (1 - (cv_r2 / max(cv_r2))) +
                 0.05 * (predictors_count / max(predictors_count)))

# I selected the best-performing formula based on this custom score.
best_formula_bss_actual <- as.formula(bss_results_actual$formula[which.min(bss_results_actual$score)])
model_bss_actual <- lm(best_formula_bss_actual, data = train_val_data)

# I made predictions on the holy test set using this selected model.
pred_bss_actual_holy <- predict(model_bss_actual, newdata = holy_test_data)

# I then evaluated performance on both CV and holy test data.
mse_bss_holy_actual <- mean((holy_test_data$car_price - pred_bss_actual_holy)^2)
r2_bss_holy_actual <- cor(holy_test_data$car_price, pred_bss_actual_holy)^2

# And finally saved the lowest CV MSE and corresponding R² for the best-scoring model.
mse_bss_cv_actual <- min(bss_results_actual$cv_mse)
r2_bss_cv_actual <- bss_results_actual$cv_r2[which.min(bss_results_actual$score)]

```

```{r}
# I printed the best formula selected by the custom scoring strategy.
best_formula_string <- bss_results_actual$formula[which.min(bss_results_actual$score)]
cat("Best Subset Formula:\n", best_formula_string)

```

# Results of the Best Subset Selection

```{r}
# I created a summary table to clearly display performance of the Best Subset model.
# I kept the focus only on actual prices, as that's our main evaluation target.

bss_summary_table <- data.frame(
  Version = "Actual Price",  
  Evaluation = c("Cross-Validation (5-Fold)", "Holy Test Set"),  
  RMSE = c(sqrt(mse_bss_cv_actual), sqrt(mse_bss_holy_actual)),  
  R2 = c(r2_bss_cv_actual, r2_bss_holy_actual)
)

# Displayed the results in a clean and visually structured HTML table.
kable(bss_summary_table, caption = "Best Subset Selection Performance (RMSE and R²) — Actual Prices") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE, 
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "white", background = "#28a745") %>%
  row_spec(1:2, background = "#f7f7f7")

```


## Lasso

```{r}
# ----------------------------
# LASSO Regression with Cross-Validated Lambda (Actual Price Version)
# ----------------------------

# I first combined my training and validation sets to fit the Lasso model on the largest possible dataset.
train_val_data <- bind_rows(train_data, validation_data)

# Prepare input matrices for Lasso. The `model.matrix` function one-hot encodes categorical variables,
# and I removed the intercept manually with `-1`.
X_lasso_actual_train <- model.matrix(car_price ~ . -1, data = train_val_data)
Y_lasso_actual_train <- train_val_data$car_price

X_lasso_actual_test <- model.matrix(car_price ~ . -1, data = holy_test_data)
Y_lasso_actual_test <- holy_test_data$car_price

# I used 5-fold cross-validation to determine the optimal penalty strength (lambda).
# Since Lasso is a shrinkage method, this helps identify how aggressively coefficients should be penalized.
cv_lasso_actual <- cv.glmnet(X_lasso_actual_train, Y_lasso_actual_train, alpha = 1, nfolds = 5)

# I selected the lambda value that minimized the mean cross-validated error.
best_lambda_lasso_actual <- cv_lasso_actual$lambda.min

# Fit the final Lasso model using the best lambda on the full training+validation set.
model_lasso_actual <- glmnet(X_lasso_actual_train, Y_lasso_actual_train, alpha = 1, lambda = best_lambda_lasso_actual)

# Generate predictions on the untouched holy test set to evaluate out-of-sample performance.
pred_lasso_actual_test <- predict(model_lasso_actual, newx = X_lasso_actual_test)

# Compute performance metrics for the holy test set (MSE and R²).
mse_lasso_actual_test <- mean((Y_lasso_actual_test - pred_lasso_actual_test)^2)
r2_lasso_actual_test <- cor(Y_lasso_actual_test, pred_lasso_actual_test)^2

# Extract performance from the cross-validation folds as well.
mse_lasso_cv_actual <- min(cv_lasso_actual$cvm)
r2_lasso_cv_actual <- 1 - cv_lasso_actual$cvm[cv_lasso_actual$lambda == best_lambda_lasso_actual] / var(Y_lasso_actual_train)

# Summarize Lasso results in a table comparing cross-validation and test set performance.
lm_eval_tablelasso <- data.frame(
  Version = "Actual Price",
  Evaluation = c("Cross-Validation (5-Fold)", "Holy Test Set"),
  RMSE = c(sqrt(mse_lasso_cv_actual), sqrt(mse_lasso_actual_test)),
  R2 = c(r2_lasso_cv_actual, r2_lasso_actual_test)
)

# Display the performance table using kable with basic formatting
kable(lm_eval_tablelasso, caption = "Lasso Regression Performance (RMSE and R²) — Actual Prices, Cross-Validation and Holy Test Set") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE, 
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "white", background = "#0073e6") %>%
  row_spec(1:2, background = "#f2f2f2")

```

```{r}
# --- BASE PLOT VERSION ---

# I visualized the cross-validation results from cv.glmnet using the base R plot.
# This plot shows the mean cross-validated error for each lambda value.
plot(cv_lasso_actual)

# I added a vertical dashed red line to mark the log of the best lambda chosen via CV.
abline(v = log(best_lambda_lasso_actual), col = "red", lty = 2)

# Title for clarity
title("Lasso Cross-Validation Curve", line = 2.5)


# --- GGPLOT VERSION ---

# I also created a ggplot version for a more polished and customizable visual.
# First, I converted the necessary values into a dataframe.
lambda_df <- data.frame(
  log_lambda = log(cv_lasso_actual$lambda),
  cvm = cv_lasso_actual$cvm,      # mean cross-validated error
  cvsd = cv_lasso_actual$cvsd     # standard deviation of CV error
)

# Here's the ggplot-based cross-validation curve:
ggplot(lambda_df, aes(x = log_lambda, y = cvm)) +
  geom_line(color = "#0073e6") +  # CV error curve
  geom_ribbon(aes(ymin = cvm - cvsd, ymax = cvm + cvsd), alpha = 0.2) +  # Shaded region for ±1 std error
  geom_vline(xintercept = log(best_lambda_lasso_actual), color = "red", linetype = "dashed") +  # Highlight best lambda
  labs(
    title = "Lasso Cross-Validation Curve",
    x = "Log(Lambda)",
    y = "Mean Cross-Validated Error"
  ) +
  theme_minimal()

# Finally, I printed the best lambda value.
cat("Best lambda selected via cross-validation:", best_lambda_lasso_actual, "\n")

```


## Ridge

```{r}
# ----------------------------
# Ridge Regression (α = 0)
# ----------------------------

# I start by preparing the design matrices for training and holy test sets.
# The "-1" removes the intercept term since glmnet adds it internally.
X_ridge_actual_train <- model.matrix(car_price ~ . -1, data = train_val_data)
Y_ridge_actual_train <- train_val_data$car_price

X_ridge_actual_test <- model.matrix(car_price ~ . -1, data = holy_test_data)
Y_ridge_actual_test <- holy_test_data$car_price

# I apply 5-fold cross-validation to find the optimal regularization strength (lambda) for Ridge.
# Here, alpha = 0 enforces Ridge (L2 penalty).
cv_ridge_actual <- cv.glmnet(X_ridge_actual_train, Y_ridge_actual_train, alpha = 0, nfolds = 5)
best_lambda_ridge_actual <- cv_ridge_actual$lambda.min  # This is the best lambda chosen via CV

# With the optimal lambda selected, I fit the final Ridge model using the entire training set.
model_ridge_actual <- glmnet(X_ridge_actual_train, Y_ridge_actual_train, alpha = 0, lambda = best_lambda_ridge_actual)

# I then evaluate performance on the test set for reference.
pred_ridge_actual_test <- predict(model_ridge_actual, newx = X_ridge_actual_test)
mse_ridge_test_actual <- mean((Y_ridge_actual_test - pred_ridge_actual_test)^2)
r2_ridge_test_actual <- cor(Y_ridge_actual_test, pred_ridge_actual_test)^2

# Then I repeat the evaluation on the holy test set to simulate unseen data performance.
X_ridge_actual_holy <- model.matrix(car_price ~ . -1, data = holy_test_data)
Y_ridge_actual_holy <- holy_test_data$car_price

pred_ridge_actual_holy <- predict(model_ridge_actual, newx = X_ridge_actual_holy)
mse_ridge_holy_actual <- mean((Y_ridge_actual_holy - pred_ridge_actual_holy)^2)
r2_ridge_holy_actual <- cor(Y_ridge_actual_holy, pred_ridge_actual_holy)^2

# Lastly, I store the best cross-validated performance values (lowest RMSE and its corresponding R²).
mse_ridge_cv_actual <- min(cv_ridge_actual$cvm)
r2_ridge_cv_actual <- 1 - cv_ridge_actual$cvm[cv_ridge_actual$lambda == best_lambda_ridge_actual] / var(Y_ridge_actual_train)

```

```{r}

# I now format the results into a clean summary table using kable.
# This allows me to compare Ridge performance on both CV and holy test data.
ridge_table <- data.frame(
  Version = "Price", 
  Evaluation = c("Cross-Validation (5-Fold)", "Holy Test Set"),
  RMSE = c(sqrt(mse_ridge_cv_actual), sqrt(mse_ridge_holy_actual)),  # RMSE values
  R2 = c(r2_ridge_cv_actual, r2_ridge_holy_actual)  # R² values
)

# I print the table in HTML format with a clean design and a blue header for Ridge.
ridge_table %>%
  kable("html", caption = "Ridge Regression Performance (RMSE and R²)", align = "lcc") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE, position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "white", background = "steelblue3")

```


```{r}
# --- BASE PLOT VERSION (RIDGE) ---

# I visualized the cross-validation curve for Ridge using base R's default glmnet plot.
# This helps me inspect how the CV error changes across log(lambda) values.
plot(cv_ridge_actual)

# I added a vertical dashed line to highlight the log of the best lambda selected by CV.
abline(v = log(best_lambda_ridge_actual), col = "red", lty = 2)

# Title for the plot
title("Ridge Cross-Validation Curve", line = 2.5)

# --- GGPLOT VERSION (RIDGE) ---

# I used ggplot2 to plot the CV error and highlight the optimal lambda.
# First, I converted the lambda values and corresponding CV errors into a dataframe.
lambda_df_ridge <- data.frame(
  log_lambda = log(cv_ridge_actual$lambda),
  cvm = cv_ridge_actual$cvm,      # Mean CV error
  cvsd = cv_ridge_actual$cvsd     # Standard deviation of CV error
)

# Now I build the ggplot version:
ggplot(lambda_df_ridge, aes(x = log_lambda, y = cvm)) +
  geom_line(color = "#009999") +  # Ridge-themed curve color
  geom_ribbon(aes(ymin = cvm - cvsd, ymax = cvm + cvsd), alpha = 0.2) +  # Shaded uncertainty band
  geom_vline(xintercept = log(best_lambda_ridge_actual), color = "red", linetype = "dashed") +  # Best lambda marker
  labs(
    title = "Ridge Cross-Validation Curve",
    x = "Log(Lambda)",
    y = "Mean Cross-Validated Error"
  ) +
  theme_minimal()

# Finally, I printed the best lambda value.
cat("Best lambda selected for Ridge via cross-validation:", best_lambda_ridge_actual, "\n")

```

# Comparison of Linear Models

```{r}
# --- MODEL PERFORMANCE COMPARISON: LINEAR vs BEST SUBSET vs LASSO vs RIDGE ---

# I first created a summary dataframe that holds the key performance metrics
# for each model based on 5-fold cross-validation using the actual price scale.
cv_actual_results <- data.frame(
  Model = c("Linear", "Best Subset", "Lasso", "Ridge"),
  RMSE = c(
    sqrt(lm_mse_cv_actual),        # Linear Regression
    sqrt(mse_bss_cv_actual),       # Best Subset Selection
    sqrt(mse_lasso_cv_actual),     # Lasso Regression
    sqrt(mse_ridge_cv_actual)      # Ridge Regression
  ),
  R2  = c(
    lm_r2_cv_actual,
    r2_bss_cv_actual,
    r2_lasso_cv_actual,
    r2_ridge_cv_actual
  )
)

# --- RMSE Bar Chart ---

# To visually compare model accuracy, I plotted RMSE for each model.
# Lower RMSE indicates better predictive performance on the actual price scale.
ggplot(cv_actual_results, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Cross-Validated RMSE (Actual Price)",
    y = "RMSE"
  ) +
  theme_minimal() +
  theme(legend.position = "none")  # No need for a legend since bars are labeled directly

# --- R² Bar Chart ---

# I also plotted R² values to assess how well each model explains variance in the actual car prices.
# Higher R² means better model fit. This helps me understand model performance from a complementary angle.
ggplot(cv_actual_results, aes(x = Model, y = R2, fill = Model)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Cross-Validated R² (Actual Price)",
    y = "R²"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

```


# Advanced and Compact Comparison of Linear Models
```{r}
# --- ADVANCED PERFORMANCE COMPARISON: Dual-Axis Visualization for RMSE and R² ---

library(ggplot2)

# I created a compact data frame that holds both RMSE and R² values for each linear model.
# To make them comparable on one y-axis, I scaled R² values by 10,000 — this allows dual-axis plotting later on.
results_dflinear <- data.frame(
  Model = rep(c("Linear", "Best Subset", "Lasso", "Ridge"), each = 2),
  Metric = rep(c("RMSE", "R²"), times = 4),
  Value = c(
    sqrt(lm_mse_cv_actual), 10000 * lm_r2_cv_actual,
    sqrt(mse_bss_cv_actual), 10000 * r2_bss_cv_actual,
    sqrt(mse_lasso_cv_actual), 10000 * r2_lasso_cv_actual,
    sqrt(mse_ridge_cv_actual), 10000 * r2_ridge_cv_actual
  )
)

# This plot allows me to visually compare both RMSE and R² for each linear model in one unified chart.
# RMSE is plotted on the left y-axis, while R² is scaled and plotted on the right for interpretability.
ggplot(results_dflinear, aes(x = Model)) +
  geom_bar(aes(y = Value, fill = Metric), stat = "identity", 
           position = position_dodge(width = 0.8), width = 0.6) +

  # Set up dual y-axes: left for RMSE and right for rescaled R²
  scale_y_continuous(
    name = "RMSE",  # Left y-axis title
    breaks = seq(0, 10000, by = 500),  # More granularity on RMSE ticks
    sec.axis = sec_axis(~ . / 10000, name = "R²", breaks = seq(0, 1, by = 0.05))  # Right y-axis for R²
  ) +

  # Plot title and axis labels
  labs(title = "Linear Model Performance Comparison", x = "Model") +

  # Aesthetic settings for a clean and intuitive design
  theme_light(base_size = 14) +
  theme(
    panel.grid.major.y = element_line(color = "#D3D3D3", size = 0.5),
    panel.grid.minor.y = element_line(color = "#EAEAEA", size = 0.25),
    axis.title.y.right = element_text(color = "purple", size = 14), 
    axis.text.y.right = element_text(color = "purple", size = 10),
    axis.title.y.left = element_text(color = "orange", size = 14),
    axis.text.y.left = element_text(color = "orange", size = 10),
    axis.title.x = element_text(size = 14), 
    axis.text.x = element_text(size = 12),
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    legend.position = "top"
  ) +

  # Manually assign fill colors for clarity and visual consistency
  scale_fill_manual(values = c("RMSE" = "orange", "R²" = "purple"))

```

# Table for Comparison of RMSE and R^2 Values of Linear Models
```{r}
library(kableExtra)

# I first prepare a data frame containing the performance metrics (RMSE and R²)
# for each of the linear models. I round RMSE to 1 decimal and R² to 3 decimals for clarity.
linear_model_perf <- data.frame(
  Model = c("Linear", "Best Subset", "Lasso", "Ridge"),
  RMSE = c(
    round(sqrt(lm_mse_cv_actual), 1),           # Root Mean Squared Error
    round(sqrt(mse_bss_cv_actual), 1),
    round(sqrt(mse_lasso_cv_actual), 1),
    round(sqrt(mse_ridge_cv_actual), 1)
  ),
  R2 = c(
    round(lm_r2_cv_actual, 3),                  # R-squared
    round(r2_bss_cv_actual, 3),
    round(r2_lasso_cv_actual, 3),
    round(r2_ridge_cv_actual, 3)
  )
)

# I use kableExtra to generate a clean and professional HTML table.
# This table summarizes model performance in a format suitable for reports.
linear_model_perf %>%
  kable("html", caption = "Linear Model Performance (RMSE and R²)", align = "c") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),  # Compact, readable style
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "white", background = "#4E79A7") %>%    # Header style
  row_spec(1:4, background = "#f7f7f7")                                     # Light gray rows for clarity

```
